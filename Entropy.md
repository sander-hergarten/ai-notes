The entropy $H_X$ of a discrete random variable $X$ with probability distribution $p(x)$ is defined as 
$$H_X\equiv -\sum_{x\in\mathcal X}p(x)\log_2p(x)=\mathbb E\log_2 \left[\frac{1}{p(X)}\right]$$
where we define by continuity $0\ \log_2 0=0$. We shall also use the notation $H(p)$ whenever we want to stress the dependence of the entropy upon the probability distribution of $X$.

We us the logarithm to the base $2$, which is well adapted to digital communication and the entropy is then expressed 