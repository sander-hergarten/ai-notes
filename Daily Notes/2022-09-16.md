# Hyper Transformer: Policy Embeddings
## Introduction
Many reinforcement learning approaches suffer from poor cross-domain generalization and require a large amount of runs to converge on a acceptable policy or Q-function. 

Traditional policy optimization algorithms attempt to optimize for an optimal policy across the weights. This results in a semi-optimal strategy to be described by the networks architecture directly and as such does not involve embedding of the actual policy. Embedding would require a understanding of the networks self-optimization which is why a hypernetwork would be of benefit. 

## Hypertransformers
The Hypertransformer is a recent development in the space created for image recoginition tasks. Given an image and its labels it is possible for the transformer to create key value pairs in a fashion where datapoints combined with the previous layers weights produce new weights for the next layer. Being able to classify the given task and adjust the network is a benefit Hypertransformers offer that could be of benefit in reinforcement learning agents.

## Environment embeddings
### Describing environments
An environment can be abstracted as a markov chain of arbitrary size. In this 

### Trajectories
All possible runs through the markov chain are the set of all possible trajectories.  As such a full description of the environment could be provided by the set of all states and a function that accepts the current state, the action, and the reward, and returns the transitional  to the next states. 