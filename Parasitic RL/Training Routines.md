# Offline Routines
## Base Routine
This routine trains 
- decoder 
- continue predictor 
- reward predictor 
- dynamcs predictor 
- encoder 
- sequence model alpha

using samples of 

(observation, reward, action, continue_flag)

The model tries to predict:
- reward 
- continue 
- observation
and gets evaluated using the appropriate losses

## High Reward Routine
When episode is high these models are being trained:
- sequence model beta
- encoder
- decoder
- action predictor
- dynamics predictor
- continue predictor
this is similar to the Base Routine but the recurrent state is generated by sequence model beta. 

The sequence model is being biased to adopt a similar recurrent state as sequence model alpha.

The dynamics predictor and encoder as well as the sequence model beta are required to learn a unified representation of the recurrent state as sequence model alpha generates.

The decoder and continue predictor make sure that the recurrent generated is contextually similar as well and encodes a defined environment state. 

The action predictor learns to translate the current model state to an action that could be represented as q-values. 

This routine receives:

(observation, action, continue, gru_vector_alpha)

## Low Reward Routine
The low reward routine abuses the fact that the action predictor should not build an internal world representation but act more as a translator

As such this routine will train
- action predictor 

The action predictor receives the model state of the base routine (recurrent state, stochastic state) and tries to predict the action. 

this routine works with samples of:

(recurrent_state_alpha, stochastic_state_alpha, action)

# Autoregressive Routines
Autoregressive routines work if it is possible to construct a classic RL algorithm out of the sequence model and the action predictor. 

## Autoregressive Base Routine
Sequence model alpha begins with 5 steps in an environment, similar as in the Dreamer Paper. After that sequence model betas branch begins predicting actions which sequence model alpha evaluates and gives rewards.

The encoder, when sequence model beta is predicting, acts upon the observation generated by the decoder from sequence model alphas evaluation of the prior step. sequence model alpha uses the dynamics predictor to predict the next stochastic state.

## Autoregressive Blind Routine
Very similar to the base routine just that sequence model beta does not use the generated observation but instead tries to predict the next state using the dynamics predictor. This routine requires sequence model beta to have a good understanding of the environment 

# Online Routine
The online routine combines all of the routines. At the beginning of training, sequence model beta directly acts upon the environment to generate the samples that sequence model alpha learns from.  Sequence model beta will improve with the chosen reinforcement learning strategy and will also be optimised by the high reward routine and the action predictor by the low reward routine

Since early during training high rewards will be sparse high reward routines are determined by a moving metric which decides the threshold for high and low reward routines. In addition when rewards are very sparse sequence model alpha and betas representations could stray to far from each other. By using strong regularisation and overfitting on the small sample size sequence model beta could be forced to relearn its policy to be more in line with sequence model alpha.

Once sequence model alpha has been trained sufficiently it is possible to mix in more and more autoregressive training. 

Blind autoregressive training can be used to test the accuracy of internal representation of sequence model beta and could be used as a metric to decide which routines to increase/decrease.

Using partially blind online training can also be used as a metric to determine if the representation the sequence models acquired fit the environment. This is dependent on how much randomness the environment contains