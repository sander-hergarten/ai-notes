---
title: First-Explore, then Exploit: Meta-Learning Intelligent Exploration
authors: Ben Norman, Jeff Clune
year: 2023
---
In standard [[Reinforcement Learning|RL]], the same [[Policies|policy]] is generally used for two different purposes:
1. Exploring: gathering data to improve the policy 
2. Exploitation: using the gathered data to specify a highly performant policy

Exploring by relying on "noisy exploitation", will never solve some tasks. 

To avoid local optima, requires effective exploring which requires sacrificing episode reward during exploration. This is a general principle:
- **Sacrificial exploration**: exploration that is not exploitative is scrificial as one is 'sacrificing' episode reward for information gain
- **Coincidental exploration:** exploitation that happens by coincidence when noisily exploiting. Relying on coincidental exploration is the standard RL approach, and is vulnerable to local optima

Standard RL never intentionally sacrificially explores because each episode is spent trying to maximize reward. This inability prevents standard RL from optimally exploring, and so causes greater sample inefficiency, making solving hostile tasks infeasible.

## Memory-less Exploration
People often exhaustively explore. In standard RL, an agent has no knowledge or memory of previous episodes, and so it will do approximately the same exploration repeatedly. This lack of memory can make the standard RL exploration very sample inefficient.

## No Prior on Exploration
Effective and efficient exploration requires a prior on how to explore in the environment. 

Furthermore, a good exploration prior is often different from a good exploitation prior because optimal exploration often requires sacrificing episode reward.

## Meta-RL
[[Meta Reinforcement Learning]] attempts to address many of standard RL's issues by learning a reinforcement learning algorithm. This reinforcement learning algorithm can be realized as a mapping from a context of rollouts $c$ in an environment $m$ to a performant policy $\pi_{\theta,c}$ spezialized to that environment, whether by a transformer, recurrent network or other method capable of processing long-sequences or memory. To train meta-RL, one specifies a distribution of environments $\mathcal M$. Giving the agent multiple interactions with a sampled environment then enables the policy to lear n to adapt to the specifics of the environment $m$ it is in, and also make us of and learn the prior that the environment come from the training distribution $m\sim \mathcal M$ 

Meta RL agents still suffer from exploitation vs exploration

