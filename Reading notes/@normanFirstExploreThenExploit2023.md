---
title: First-Explore, then Exploit: Meta-Learning Intelligent Exploration
authors: Ben Norman, Jeff Clune
year: 2023
---
# Abstract 
In standard [[Reinforcement Learning|RL]], the same [[Policies|policy]] is generally used for two different purposes:
1. Exploring: gathering data to improve the policy 
2. Exploitation: using the gathered data to specify a highly performant policy

solution is to train one policy that exploits a situation, and one policy that explores. The explore policy gets rewarded based on how the exploit situation is rewarded after the explore.

# Related Works
Exploring by relying on "noisy exploitation", will never solve some tasks. 

To avoid local optima, requires effective exploring which requires sacrificing episode reward during exploration. This is a general principle:
- **Sacrificial exploration**: exploration that is not exploitative is scrificial as one is 'sacrificing' episode reward for information gain
- **Coincidental exploration:** exploitation that happens by coincidence when noisily exploiting. Relying on coincidental exploration is the standard RL approach, and is vulnerable to local optima

Standard RL never intentionally sacrificially explores because each episode is spent trying to maximize reward. This inability prevents standard RL from optimally exploring, and so causes greater sample inefficiency, making solving hostile tasks infeasible.

## Memory-less Exploration
People often exhaustively explore. In standard RL, an agent has no knowledge or memory of previous episodes, and so it will do approximately the same exploration repeatedly. This lack of memory can make the standard RL exploration very sample inefficient.

## No Prior on Exploration
Effective and efficient exploration requires a prior on how to explore in the environment. 

Furthermore, a good exploration prior is often different from a good exploitation prior because optimal exploration often requires sacrificing episode reward.

## Meta-RL
[[Meta Reinforcement Learning]] attempts to address many of standard RL's issues by learning a reinforcement learning algorithm. This reinforcement learning algorithm can be realized as a mapping from a context of rollouts $c$ in an environment $m$ to a performant policy $\pi_{\theta,c}$ spezialized to that environment, whether by a transformer, recurrent network or other method capable of processing long-sequences or memory. To train meta-RL, one specifies a distribution of environments $\mathcal M$. Giving the agent multiple interactions with a sampled environment then enables the policy to lear n to adapt to the specifics of the environment $m$ it is in, and also make us of and learn the prior that the environment come from the training distribution $m\sim \mathcal M$ 

Meta RL agents still suffer from exploitation vs exploration

## Other Works Addressing Exploration

There is a rich literature on non-meta-RL exploration approaches. One relevant approach is Intrinsic Motivation (IM), which replaces the environment reward with an intrinsic motivation reward such as novelty [[@aubretSurveyIntrinsicMotivation2019]]. Despite the success of IM at enabling sacrificial exploration, these methods are limited by being slow to adapt due to lacking a memory not encoded via weights and not having a complex learnt prior. Another deeper problem is that many of these methods enable sacrificial exploration by entirely ignoring the reward signal, leading to pathologies such as the noisy TV problem[[@burdaExplorationRandomNetwork2018]] where an agent looking for new states will find a TV showing white noise to be endlessly captivating. 


# First explore Framework

First explore learns a pair of policies. An explore policy $\pi_{\text{explore},\theta,c}$ that explores and provides information for itself and for exploitation, and an exploit policy $\pi_{\text{exploit},\theta,c}$ that exploits after every explore providing feedback to train the explore policy. 

The policies may share or have separate parameters. 

The eplore episodes $\tau_1,\tau_{2},...$ are sampled using the explore policy $\pi_{\text{explore},\theta,c}$ and the context of previous explore episodes, e.g. for $\tau_t,c=\set{\tau_1,..,\tau_{t-1}}$. These explore episodes are purely exploratory and are able to sacrificially explore.

The exploit episodes $\tau_1',\tau_2',...$ are sampled using the exploit policy $\pi_{\text{exploit},\theta,c}$ and the context of all preceding explore episodes. They are purely exploitative.

Each policy is limited by the quality of the other, as if there is no useful context then an excellent exploit policy will do no better than a mediocre one, and if the exploit policy is poor then mediocre and excellent context will be similarly indistinguishable. Thus for complex tasks, the policies need to be learnt together.

A central idea of the First explore is that the exploratory value $v_{\text{explore}}$ of an explore episode $\tau_\text{explore}$ given a context of past episodes $\set{\tau_1,...,\tau_n}$ is the increase in expected reward of a subsequent exploit when the explore episode is added to the context to create new context $\set{\tau_1,...,\tau_n,\tau_\text{explore}}$.
$$v_\text{explore}(\tau_\text{explore})|\set{\tau_1,...,\tau_{n}}=\mathbb E(\tau_\text{exploit}|\set{\tau_1,...,\tau_n,\tau_\text{explore}})- E(\tau_\text{exploit}|\set{\tau_1,...,\tau_n})$$
where $\tau_\text{exploit}|c$ denotes a rollout from $\pi_\text{exploit},\theta,c$

As the last term does not depend on the explore episode, it is possible to discard the last term when learning the optimal exploration policy. The reward function for the explore policy is thus the reward of the following exploit.

To train, one then iterates building a context exclusively from exploration, each time determining the value of each exploration by a subsequent exploit. Evaluating and crediting each explore in this way allows First-Explore to avoid the value assignment problem of E-RL$^2$. First-explore can be combined with different meta-RL approaches and losses. 

Once the two policies have been trained, to adapt to an environment, one performs iterated rollouts in the environment using the explore policy to sample-efficiently explore, with each. explore rollout added to the context potentially improving the context conditioned exploit policy. 

This process is the analogue of sample-efficiently training a standard RL policy where accumulation of informative context replaces standard RL-training, and exploit rollouts replace standard RL evaluation rollouts.

One approach would be to explore in the environment until a preset desired exploit quality is reached. One could also do a set number of rollouts


![[Pasted image 20230731111134.png]]
![[Pasted image 20230731111148.png]]
