---
title: Human-Timescale Adaptation in an Open-Ended Task Space
authors:  Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreya Pathak, Nicolas Perez-Nieves, Nemanja Rakicevic, Tim Rockt√§schel, Yannick Schroecker, Jakub Sygnowski, Karl Tuyls, Sarah York, Alexander Zacherl, Lei Zhang
year: 2023
---
Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning. This paper attempts to demonstrate an agent that displays on-the-fly hypothesis driven exploration, efficient exploitation of acquired knowledge and can be prompted with first person demonstrations.

Adaptation emerges from three ingredients:
1. meta-reinforcement learning across a vast, smooth and diverse task distribution,
2. a policy parameterised as a large-scale  attention based memory architecture
3. an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities.

# Introduction 
Tranformers are used as an architecutral choice to scale in-context fast adaptation via model-base RL$^2$ 




